{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U imbalanced-learn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-learn imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20914,
     "status": "ok",
     "timestamp": 1700439405736,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "_9NZbmEfFWEs",
    "outputId": "f4c0ece1-214d-408f-d6d9-468c4c2e3fd1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tweepy as tw\n",
    "import re    # it is RegEx useed to remove non-letter characters\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# For Building the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "#For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "\n",
    "!pip install emoji\n",
    "\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_vmAxq-FPAe"
   },
   "source": [
    "## Fetching the newly created merged dataset using a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11213,
     "status": "ok",
     "timestamp": 1700455830160,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "c1A7In8TFFjk"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('merged_raw_dataset.csv').sample(n=100000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1700455830161,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "k627cilH_XIA",
    "outputId": "83650c5f-f705-46ea-a5c9-5802f5b9c8ac"
   },
   "outputs": [],
   "source": [
    "# Number of records in the dataset\n",
    "row_count = df1.shape\n",
    "print(\"Number of Records in the raw dataset: \",row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChVZ3d-HFnlT"
   },
   "source": [
    "# Finding the unique App Domains in the Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1700455830161,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "E-BVawaJFqa3",
    "outputId": "0c33cbe9-a28a-4795-e979-be6f2aa0ef63"
   },
   "outputs": [],
   "source": [
    "unique_app_domains = df1['App Domain'].unique()\n",
    "\n",
    "# Print or use the unique values\n",
    "print(unique_app_domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsL6t6BMFw94"
   },
   "source": [
    "# Finding the unique App names in the merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1700455830161,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "lCXBFkSsFzdH",
    "outputId": "01f304e5-f580-4709-eae5-ba6d8c469505"
   },
   "outputs": [],
   "source": [
    "unique_app_name = df1['AppName'].unique()\n",
    "\n",
    "# Print or use the unique values\n",
    "print(unique_app_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4Jmmuv5F5v0"
   },
   "source": [
    "# Intial Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 321,
     "status": "ok",
     "timestamp": 1700455830480,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "qp8p9NdMF2OY",
    "outputId": "3f64f7d1-a105-46cb-f7d9-65b4b4190722"
   },
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fNoLOiJF-Mt"
   },
   "source": [
    "# Intial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "executionInfo": {
     "elapsed": 962,
     "status": "ok",
     "timestamp": 1700455831438,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "pzWbLDEVF8Jv",
    "outputId": "d584b444-a2f8-4522-c76b-f22c04a17761"
   },
   "outputs": [],
   "source": [
    "df1['content'] = df1['content'].astype(str)\n",
    "# Calculate the length of each review\n",
    "df1['ReviewCharLength'] = df1['content'].apply(len)\n",
    "\n",
    "print(df1['ReviewCharLength'])\n",
    "# Plot a box plot for review lengths\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(y=df1['ReviewCharLength'])\n",
    "plt.title('Number of characters in the Review ')\n",
    "plt.xlabel('Review Character Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 905,
     "status": "ok",
     "timestamp": 1700455832338,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "WtQVYWfiGA07",
    "outputId": "bbda9aac-27cb-43b2-fbe7-fcf8844fcedf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate the length of each review\n",
    "df1['ReviewCharLength'] = df1['content'].apply(len)\n",
    "\n",
    "# Calculate histogram values\n",
    "hist, bins = np.histogram(df1['ReviewCharLength'], bins=20)\n",
    "\n",
    "# Plot a histogram for review lengths\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df1['ReviewCharLength'], bins=40, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram for the Character Count of the Reviews')\n",
    "plt.xlabel('Review Character Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 876
    },
    "executionInfo": {
     "elapsed": 13580,
     "status": "ok",
     "timestamp": 1700455845917,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "MskwXRI9GDHf",
    "outputId": "feedecae-3873-4ce2-9376-6f2f9063a070"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df1 is your DataFrame\n",
    "# Function to extract special characters from text\n",
    "def extract_special_characters(text):\n",
    "    special_characters_list = []\n",
    "    special_characters_pattern = regex.compile(r'\\p{S}')\n",
    "    special_characters = special_characters_pattern.findall(text)\n",
    "    for char in special_characters:\n",
    "        special_characters_list.append(char)\n",
    "    return special_characters_list\n",
    "\n",
    "# Apply the function to the \"content\" column in the 'df1' dataframe\n",
    "df1['special_characters'] = df1['content'].apply(extract_special_characters)\n",
    "\n",
    "# Flatten the list of special characters\n",
    "flat_special_characters_list = [item for sublist in df1['special_characters'] for item in sublist]\n",
    "\n",
    "# Count the occurrences of each special character\n",
    "special_character_counts = dict(Counter(flat_special_characters_list))\n",
    "\n",
    "# Sort the special characters by count\n",
    "sorted_special_characters = dict(sorted(special_character_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Get the top 10 most used special characters\n",
    "top_10_special_characters = dict(list(sorted_special_characters.items())[:10])\n",
    "\n",
    "# Plot the top 10 most used special characters\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_10_special_characters.keys(), top_10_special_characters.values())\n",
    "plt.title('Top 10 Most Used Special Characters in Reviews')\n",
    "plt.xlabel('Special Character')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Find the special character with the highest count\n",
    "highest_count_character = max(special_character_counts, key=special_character_counts.get)\n",
    "highest_count = special_character_counts[highest_count_character]\n",
    "\n",
    "# Display the result\n",
    "print(f\"The special character with the highest count is '{highest_count_character}' with a count of {highest_count}\")\n",
    "\n",
    "# Display all top 10 characters and their counts\n",
    "print(\"Top 10 Special Characters:\")\n",
    "for char, count in top_10_special_characters.items():\n",
    "    print(f\"{char}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1700455845917,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "SN1sWJz1GISo",
    "outputId": "9ebf0dfa-8274-4fdb-d076-a941a286e0d3"
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1700455846257,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "q807bP8gGhsm",
    "outputId": "c4c0ac6b-791a-4e69-f386-68f4c6e32730"
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis Bar Chart\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(x='score', data=df1)\n",
    "plt.title('Distribution of Customer Ratings')\n",
    "plt.xlabel('Customer Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USuFo1_8GqYK"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1700455846258,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "zO52OQvAGqw0",
    "outputId": "d5b3c935-f9cf-4f6c-c457-a402b54ab827"
   },
   "outputs": [],
   "source": [
    "# Number of records in the dataset\n",
    "row_count = df1.shape\n",
    "print(\"Number of Records in the raw dataset: \",row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3jX4kbEGvjP"
   },
   "source": [
    "#Handling Null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1854,
     "status": "ok",
     "timestamp": 1700455848109,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "KevLzBKYGtii",
    "outputId": "c50d1552-a049-4ff1-b5f6-d9dd928b576c"
   },
   "outputs": [],
   "source": [
    "#Checking the number of null values before the removal of null values in content\n",
    "\n",
    "print(\"The number of null values in the Dataset:\\n\",df1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 976,
     "status": "ok",
     "timestamp": 1700455849084,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "zScu1CuZG4mJ"
   },
   "outputs": [],
   "source": [
    "#Removing the null values in the userName Column\n",
    "df1.dropna(subset=['userName'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1616,
     "status": "ok",
     "timestamp": 1700455850698,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "fHR4v6u1HDvz",
    "outputId": "d484b8fc-3704-4937-c497-3b05bb09a697"
   },
   "outputs": [],
   "source": [
    "#Checking the number of null values before the removal of null values in content\n",
    "\n",
    "print(\"The number of null values in the Dataset:\\n\",df1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2840,
     "status": "ok",
     "timestamp": 1700455853535,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "yzzxyFi2HFdi",
    "outputId": "e12ffd45-d316-4fe5-a46e-fd452e03d952"
   },
   "outputs": [],
   "source": [
    "# Sort by 'at'\n",
    "df1= df1.sort_values('at')\n",
    "\n",
    "# Forward fill 'appVersion'\n",
    "df1['appVersion'] = df1['appVersion'].ffill()\n",
    "\n",
    "# Backward fill 'appVersion' for remaining null values\n",
    "df1['appVersion'] = df1['appVersion'].bfill()\n",
    "\n",
    "print(df1['appVersion'])\n",
    "print(\"The above values are the filled App version values  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2495,
     "status": "ok",
     "timestamp": 1700455856028,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "_NIjbMq4HHPs",
    "outputId": "df0df422-35be-42c5-df1e-17f8879a23ad"
   },
   "outputs": [],
   "source": [
    "# Sort by 'at'\n",
    "df1 = df1.sort_values('at')\n",
    "\n",
    "# Forward fill 'appVersion'\n",
    "df1['reviewCreatedVersion'] = df1['reviewCreatedVersion'].ffill()\n",
    "\n",
    "# Backward fill 'appVersion' for remaining null values\n",
    "df1['reviewCreatedVersion'] = df1['reviewCreatedVersion'].bfill()\n",
    "\n",
    "print(df1['reviewCreatedVersion'])\n",
    "print(\"The above values are the filled reviewCreated Version values  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4131,
     "status": "ok",
     "timestamp": 1700455860157,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "WnXogbuPHMAM",
    "outputId": "84a33e9b-9572-4ebb-c44f-dfbadddc2182"
   },
   "outputs": [],
   "source": [
    "#Checking the number of null values before the removal of null values in content\n",
    "\n",
    "print(\"The number of null values in the Dataset:\\n\",df1.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4707,
     "status": "ok",
     "timestamp": 1700455864841,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "Odc2YhYJHPTs"
   },
   "outputs": [],
   "source": [
    "# Impute missing text with a default NA value\n",
    "df1.fillna('NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1563,
     "status": "ok",
     "timestamp": 1700455866942,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "IAVOmfr_HRqb",
    "outputId": "2ba3cfea-e25f-4124-f751-806af67ebd3b"
   },
   "outputs": [],
   "source": [
    "#Checking the null values after the removal\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9T3StnuHUOF"
   },
   "source": [
    "# Number of Records left after removing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1700455866943,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "SwljuzyTHZG0",
    "outputId": "1e51ce4e-4321-44bf-f750-46df29a736ce"
   },
   "outputs": [],
   "source": [
    "# Count the number of rows left after removing the null values in Review content column\n",
    "row_count = df1.shape[0]\n",
    "print(\"The number of records left after removing all the null values:\", row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwQNMoQcHblo"
   },
   "source": [
    "# Handling Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1700455867296,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "cqVAyt-lHa8K",
    "outputId": "c0daf350-cf2d-48c5-b2ed-c18e1c4b8ca6"
   },
   "outputs": [],
   "source": [
    "#Checking if there duplicates in the review id column to verify whether to consider the removal of duplicate column\n",
    "print(\"Number of duplicates in the ReviewidColumn: \",len(df1['reviewId'])-len(df1['reviewId'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1370,
     "status": "ok",
     "timestamp": 1700455868663,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "YMzNVgb7HjlV",
    "outputId": "2c12b81c-948f-4f0e-b904-a6eb3454369b"
   },
   "outputs": [],
   "source": [
    "# Drop duplicates based on a particular column\n",
    "df_no_duplicates = df1.drop_duplicates(subset='reviewId',inplace=True)\n",
    "print(\"The number of duplicates left: \",df_no_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1700455868923,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "AWRgy9V5Hl0S",
    "outputId": "4579e7f9-e4af-475f-ba76-48f92e81f872"
   },
   "outputs": [],
   "source": [
    "#Check the count of duplicates in Review Content Coumn\n",
    "\n",
    "print(\"Number of duplicates in the Content Column: \",len(df1['content'])-len(df1['content'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVzrfiOxHsQg"
   },
   "source": [
    "# Number of records after removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1700455868924,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "F8CIhP8IHp32",
    "outputId": "aee74b69-40a4-400b-a47b-f45d8cd60020"
   },
   "outputs": [],
   "source": [
    "# Count the number of rows left after removing the null values in Review content column\n",
    "row_count = df1.shape[0]\n",
    "print(\"The number of records  after removing duplicates in the review id column: \",row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L68JrswlH6eM"
   },
   "source": [
    "# Handling emoji data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4707,
     "status": "ok",
     "timestamp": 1700455873619,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "MtImJMIwH-Bp"
   },
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Apply the function to the \"content\" column\n",
    "df1['content'] = df1['content'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1163,
     "status": "ok",
     "timestamp": 1700455874769,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "ZJbfDakrIR9y",
    "outputId": "33479271-7427-4cb0-825c-c203ba5acae4"
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only rows with emoji\n",
    "emoji_rows = df1[df1['content'] == True]\n",
    "\n",
    "# Print the rows with emoji\n",
    "print(emoji_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GFdWjHfJC4B"
   },
   "source": [
    "# Handling Special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2473,
     "status": "ok",
     "timestamp": 1700455877235,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "IV7u3oiXJFKg"
   },
   "outputs": [],
   "source": [
    "# Function to remove special characters from text and replace with a space\n",
    "def remove_special_characters(text):\n",
    "    # Define a regular expression pattern to match special characters\n",
    "    special_characters_pattern = r':'\n",
    "\n",
    "    # Use re.sub to replace matched special characters with a space\n",
    "    cleaned_text = re.sub(special_characters_pattern, ' ', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the function to remove special characters and replace with spaces\n",
    "df1['content'] = df1['content'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2055,
     "status": "ok",
     "timestamp": 1700455879278,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "-uCV1PWONRWx",
    "outputId": "35d97850-4852-4e27-fe1c-934052cc6522"
   },
   "outputs": [],
   "source": [
    "# Filter rows with non-empty 'removed_characters' lists (indicating special characters were removed)\n",
    "df1 = df1[df1['content'].str.len() > 0]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(df1['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1700455889734,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "QGoggZjJEWPj",
    "outputId": "3955c6e1-9ebe-40bd-e487-c7f9a591447d"
   },
   "outputs": [],
   "source": [
    "# Count the number of rows left after removing the null values in Review content column\n",
    "row_count = df1.shape\n",
    "print(\"The number of records after Data Cleaning \", row_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_cxRUABJWo7"
   },
   "source": [
    "# Data Transformation\n",
    "## Data Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 691,
     "status": "ok",
     "timestamp": 1700455970009,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "m5TyCZ--PB19",
    "outputId": "514f3dd1-0cb9-4ae3-8f3c-3eeb1c55f84a"
   },
   "outputs": [],
   "source": [
    "def classify_score(value):\n",
    "    if value <3:\n",
    "        return -1\n",
    "    elif value == 3:\n",
    "        return 0\n",
    "    elif value >3:\n",
    "        return 1\n",
    "\n",
    "df1['category'] = df1['score'].apply(classify_score)\n",
    "\n",
    "df1['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1700455971945,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "MaZNGwdVPGUA",
    "outputId": "6bfb8d1d-4035-44cd-8ee2-e4090b3b1593"
   },
   "outputs": [],
   "source": [
    "df1['category'] = df1['category'].map({-1.0:'Negative', 0.0:'Neutral', 1.0:'Positive'})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrvV8ovjPIgf"
   },
   "source": [
    "\n",
    "# Displaying Postive distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "executionInfo": {
     "elapsed": 8320,
     "status": "ok",
     "timestamp": 1700455982472,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "-8vinbYFPMoB",
    "outputId": "332c88af-bfed-4d3e-ee2f-c975ca53736c"
   },
   "outputs": [],
   "source": [
    "# Distributing all +ve sentiment tweets\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "fig = plt.figure(figsize=(14,7))\n",
    "df1['word_count'] = df1.content.str.split().apply(len)\n",
    "ax1 = fig.add_subplot(122)\n",
    "# Specify the bin size (adjust as needed)\n",
    "bin_size = 40\n",
    "\n",
    "# Use seaborn to create a histogram plot with specified bin size\n",
    "sns.histplot(df1[df1['category'] == 'Positive']['word_count'], ax=ax1, color='green', bins=bin_size)\n",
    "\n",
    "describe = df1.word_count[df1.category=='Positive'].describe().to_frame().round(2)\n",
    "\n",
    "ax2 = fig.add_subplot(121)\n",
    "ax2.axis('off')\n",
    "font_size = 14\n",
    "bbox = [0, 0, 1, 1]\n",
    "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
    "table.set_fontsize(font_size)\n",
    "fig.suptitle('Distribution of text length for positive sentiment Reviews.', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4K6GKzsPOte"
   },
   "source": [
    "# Dispalying Negative Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "executionInfo": {
     "elapsed": 4556,
     "status": "ok",
     "timestamp": 1700455987025,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "HLWbDD6pPSoY",
    "outputId": "6478303c-4281-4065-8cc8-afd715563a2c"
   },
   "outputs": [],
   "source": [
    "# Distributing all -ve sentiment tweets\n",
    "fig = plt.figure(figsize=(14,7))\n",
    "df1['word_count'] = df1.content.str.split().apply(len)\n",
    "ax1 = fig.add_subplot(122)\n",
    "# Specify the bin size (adjust as needed)\n",
    "bin_size = 40\n",
    "sns.histplot(df1[df1['category']=='Negative']['word_count'], ax=ax1,color='red',bins=bin_size)\n",
    "describe = df1.word_count[df1.category=='Negative'].describe().to_frame().round(2)\n",
    "\n",
    "ax2 = fig.add_subplot(121)\n",
    "ax2.axis('off')\n",
    "font_size = 14\n",
    "bbox = [0, 0, 1, 1]\n",
    "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
    "table.set_fontsize(font_size)\n",
    "fig.suptitle('Distribution of text length for Negative sentiment Reviews.', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPMftLL-PVTF"
   },
   "source": [
    "# Displaying Neutral Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "executionInfo": {
     "elapsed": 9695,
     "status": "ok",
     "timestamp": 1700455996716,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "AAVra8VtPaOX",
    "outputId": "77bbb399-abe0-42bb-a207-e44b551e412c"
   },
   "outputs": [],
   "source": [
    "# Distributing all -ve sentiment tweets\n",
    "fig = plt.figure(figsize=(14,7))\n",
    "df1['word_count'] = df1.content.str.split().apply(len)\n",
    "ax1 = fig.add_subplot(122)\n",
    "\n",
    "# Specify the bin size (adjust as needed)\n",
    "bin_size = 40\n",
    "sns.histplot(df1[df1['category']=='Neutral']['word_count'], ax=ax1,color='Yellow',bins=bin_size)\n",
    "describe = df1.word_count[df1.category=='Neutral'].describe().to_frame().round(2)\n",
    "\n",
    "ax2 = fig.add_subplot(121)\n",
    "ax2.axis('off')\n",
    "font_size = 14\n",
    "bbox = [0, 0, 1, 1]\n",
    "table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)\n",
    "table.set_fontsize(font_size)\n",
    "fig.suptitle('Distribution of text length for Neutral sentiment Reviews.', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcxAqKvSP_S0"
   },
   "source": [
    "# Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "executionInfo": {
     "elapsed": 53755,
     "status": "ok",
     "timestamp": 1700456051176,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "M4BuuvoGP8JJ",
    "outputId": "73722d01-e188-4390-d5d8-e38708d6f7fb"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "def wordcount_gen(df, category):\n",
    "    '''\n",
    "    Generating Word Cloud\n",
    "    inputs:\n",
    "       - df: tweets dataset\n",
    "       - category: Positive/Negative/Neutral\n",
    "    '''\n",
    "    # mixing all tweets\n",
    "    combined_tweets = \" \".join([tweet for tweet in df[df1.category==category]['content']])\n",
    "\n",
    "    # starting wordcloud object\n",
    "    wc = WordCloud(background_color='white',\n",
    "                   max_words=50,\n",
    "                   stopwords = STOPWORDS)\n",
    "\n",
    "    # Create and plot wordcloud\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(wc.generate(combined_tweets))\n",
    "    plt.title('{} Sentiment Words'.format(category), fontsize=30,color='black')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# +ve tweet words\n",
    "wordcount_gen(df1, 'Positive')\n",
    "\n",
    "# -ve tweet words\n",
    "wordcount_gen(df1, 'Negative')\n",
    "\n",
    "# Neutral tweet words\n",
    "wordcount_gen(df1, 'Neutral')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1700456051176,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "eEUVZ4ElC4L9",
    "outputId": "c17d9415-bbe9-4c4b-f861-0bd32b33c59f"
   },
   "outputs": [],
   "source": [
    "# Count the number of rows left after removing the null values in Review content column\n",
    "row_count = df1.shape\n",
    "print(\"The number of records  after removing duplicates in the review id column: \",row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of viewing the processed content\n",
    "print(df1[['content']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVCzMuAqiSqx"
   },
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2998,
     "status": "ok",
     "timestamp": 1700456102459,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "K687qhVMXQEO",
    "outputId": "d107c6e1-d79f-4a0e-f8d4-5f07d7fc1aed"
   },
   "outputs": [],
   "source": [
    "# English alphabets\n",
    "english_alphabets = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# Function to check if the string starts with an English character\n",
    "def starts_with_english(s):\n",
    "    if isinstance(s, str):\n",
    "        words = s.split()\n",
    "        if words:\n",
    "            return words[0][0] in english_alphabets\n",
    "    return False\n",
    "\n",
    "# Ensure 'content' column exists\n",
    "if 'content' in df1.columns:\n",
    "    # Filtering the DataFrame\n",
    "    df1 = df1[df1['content'].apply(starts_with_english)]\n",
    "\n",
    "# Displaying the first few rows of the filtered DataFrame\n",
    "    print(df1.head())\n",
    "else:\n",
    "    print(\"The 'content' column does not exist in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of viewing the processed content\n",
    "print(df1[['content']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk_data = ['punkt', 'wordnet', 'stopwords']\n",
    "for data in nltk_data:\n",
    "    nltk.download(data, quiet=True)\n",
    "\n",
    "def tweet_to_words(tweet):\n",
    "    if not isinstance(tweet, str):\n",
    "        return []  # Return an empty list for non-string input\n",
    "\n",
    "    # Lowercase the text\n",
    "    text = tweet.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "    \n",
    "    # Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Example DataFrame initialization (hypothetical)\n",
    "# df1 = pd.DataFrame({\n",
    "#     'content': ['Your tweet content here', 'Another tweet content'],\n",
    "#     'category': ['Category1', 'Category2']\n",
    "# })\n",
    "\n",
    "# Apply the function to the 'content' column\n",
    "df1['processed_content'] = df1['content'].apply(tweet_to_words)\n",
    "\n",
    "# Safely accessing the first row\n",
    "if not df1.empty:\n",
    "    print(\"\\nOriginal tweet ->\", df1['content'].iloc[0])\n",
    "    print(\"\\nProcessed tweet ->\", df1['processed_content'].iloc[0])\n",
    "\n",
    "# Applying data processing to individual data\n",
    "X = df1['processed_content'].apply(tweet_to_words).tolist()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(df1['category'])\n",
    "\n",
    "# Displaying the processed content and category for the first row, if available\n",
    "if len(X) > 0:\n",
    "    print(\"\\nProcessed Content of the first tweet:\", X[0])\n",
    "    print(\"Category of the first tweet:\", Y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1700457736035,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "Z1AGVzLElbt2",
    "outputId": "a67e0815-38db-4b16-af9e-f5f602d73f3e"
   },
   "outputs": [],
   "source": [
    "# Number of records in the dataset\n",
    "row_count = df1.shape\n",
    "print(\"Number of Records after Data Transformation \",row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of viewing the processed content\n",
    "print(df1[['content']].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRRspiOygPXy"
   },
   "source": [
    "# Handling Inconsistent Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-rspgnzgSv_"
   },
   "source": [
    "### Checking User Rating and Review Mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16129,
     "status": "ok",
     "timestamp": 1700457768096,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "VX1WPMfXgVJn"
   },
   "outputs": [],
   "source": [
    "#Check for mismatched ratings and comments (where we have a 5-point rating scale)\n",
    "df1['rating_comment_mismatch'] = df1.apply(\n",
    "    lambda row: (row['score'] > 3 and 'worst' in row['content']) or\n",
    "                (row['score'] < 3 and 'good' in row['content']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1700457769138,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "qPb3fouhgah6",
    "outputId": "c029c49f-22b2-4579-9a70-cf4e395b3676"
   },
   "outputs": [],
   "source": [
    "#To check the count of rating mismatch\n",
    "print(\"The number of mismatch between the Review content and Reviewer Score:\",df1['rating_comment_mismatch'].value_counts()[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1700457769138,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "3Y1L6YUugc8N",
    "outputId": "306d7061-4ce9-413c-f68b-c6cf5ff34e53"
   },
   "outputs": [],
   "source": [
    " #Checking the rating mismatch\n",
    "print(df1['rating_comment_mismatch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQlCb-4ZgfT4"
   },
   "source": [
    "## Inconsistent datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1700457769138,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "jBkAYuIjgf95",
    "outputId": "36425d0d-2522-498a-b704-858d9ed9de97"
   },
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8QmCMtmglMX"
   },
   "source": [
    "## Converting 'At' Column representing Timestamp of Review to Standard Date & Time Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1827,
     "status": "ok",
     "timestamp": 1700457770963,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "Za9gJbzdgi7c"
   },
   "outputs": [],
   "source": [
    "# Convert date to a standard format (e.g., YYYY-MM-DD)\n",
    "df1['at'] = pd.to_datetime(df1['at'], errors='coerce')  # 'coerce' will set invalid parsing as NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1700457770963,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "9xLZmuXXgovC",
    "outputId": "b87b820e-d96d-4943-90e2-52d529e7ce26"
   },
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVbvgNlrgs90"
   },
   "source": [
    "##Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1700457771366,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "rHf8F2ihgtof"
   },
   "outputs": [],
   "source": [
    "# Removing outliers\n",
    "rating_mean = df1['score'].mean()\n",
    "rating_std = df1['score'].std()\n",
    "df1 = df1[(df1['score'] > rating_mean - 3 * rating_std) & (df1['score'] < rating_mean + 3 * rating_std)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1700457771366,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "rh4X_r0Lgvwf",
    "outputId": "89444ba3-7e5f-4aaf-be7d-547c592fb814"
   },
   "outputs": [],
   "source": [
    "#Check the unique values in Score Column\n",
    "unique_values = df1['score'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJYcXc74joea"
   },
   "source": [
    "#Handling  HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1983,
     "status": "ok",
     "timestamp": 1700457777559,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "OxgpuBF4jtEI"
   },
   "outputs": [],
   "source": [
    "#Handling HTML Tags (if applicable)\n",
    "df1['content'] = df1['content'].str.replace('<[^<]+?>', '', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3034,
     "status": "ok",
     "timestamp": 1700457780591,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "eHegPqnCmRJT",
    "outputId": "596c5529-7b74-4298-9446-ff1d8a1d7b0f"
   },
   "outputs": [],
   "source": [
    "def has_html_tags(text):\n",
    "    # Define a regular expression pattern for HTML tags\n",
    "    html_tags_pattern = re.compile(r'<.*?>')\n",
    "\n",
    "    # Use findall to get all matches of the pattern in the text\n",
    "    html_tags = re.findall(html_tags_pattern, text)\n",
    "\n",
    "    # Check if any HTML tags were found\n",
    "    return bool(html_tags)\n",
    "\n",
    "# Apply the function to each row of the DataFrame\n",
    "df1['has_html_tags'] = df1['content'].apply(has_html_tags)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Is there any HTML Tags in the Text?:\\n\",df1['has_html_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of viewing the processed content\n",
    "print(df1[['content']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1700458308275,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "IhM1D9JdmWmk",
    "outputId": "10b6c08f-63a5-44cd-c315-16c60902dcfe"
   },
   "outputs": [],
   "source": [
    "# Number of records in the dataset\n",
    "row_count = df1.shape\n",
    "print(\"Number of Records after Data Transformation \",row_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2546,
     "status": "ok",
     "timestamp": 1700457845787,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "1fWxSpnALukc",
    "outputId": "c23d97fb-dde8-46f7-f89d-fb6d3a5e8752"
   },
   "outputs": [],
   "source": [
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_um-htIj4zP"
   },
   "source": [
    "#Scaling Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1402,
     "status": "ok",
     "timestamp": 1700458277644,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "deA8d-Kzj36U"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df1['ReviewCharLength'] = df1['content'].apply(len)\n",
    "scaler = MinMaxScaler()\n",
    "df1[['thumbsUpCount', 'ReviewCharLength']] = scaler.fit_transform(df1[['thumbsUpCount', 'ReviewCharLength']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1700458279263,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "s-2Zk8WrkbXv",
    "outputId": "cb5e9771-582d-49c1-e2fc-5e78c48905bd"
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_xnSQYkkYVT"
   },
   "source": [
    "#Data Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1700458281156,
     "user": {
      "displayName": "Kashish Thakur",
      "userId": "02034585595225747286"
     },
     "user_tz": 480
    },
    "id": "iWfnzf66kjey",
    "outputId": "1445e469-3e2d-46df-c409-3bf4ddbb6296"
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['Unnamed: 0','reviewId', 'userName', 'userImage', 'replyContent', 'repliedAt','special_characters','rating_comment_mismatch','has_html_tags' ]\n",
    "\n",
    "reduced_df = df1.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "reduced_df.head()\n",
    "\n",
    "column_names = reduced_df.columns.tolist()\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two columns for which you want to find covariance and correlation\n",
    "column1 = 'score'\n",
    "\n",
    "column2 = 'thumbsUpCount'\n",
    "\n",
    "# Calculate covariance\n",
    "covariance_value = df1[column1].cov(df1[column2])\n",
    "\n",
    "# Calculate correlation\n",
    "correlation_value = df1[column1].corr(df1[column2])\n",
    "\n",
    "# Display the results\n",
    "print(f'Covariance between {column1} and {column2}: {covariance_value}')\n",
    "print(f'Correlation between {column1} and {column2}: {correlation_value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSQI7o8bhiX2"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create a histogram using Seaborn\n",
    "sns.histplot(df1['score'], bins=10, kde=True, color='skyblue', edgecolor='black')\n",
    "plt.xticks([1, 2, 3, 4, 5])\n",
    "# Add labels and title\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Scores')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9n3EI6Zpherv"
   },
   "source": [
    "# EDA :Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E91OvNQEhcXL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# For example, df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = reduced_df.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Create a heatmap using Seaborn\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
    "\n",
    "# Display the heatmap\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating vocab size to create word corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Define the tweet processing function\n",
    "def tweet_to_words(tweet):\n",
    "    text = tweet.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    words = text.split()\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    words = [PorterStemmer().stem(w) for w in words]\n",
    "    return words\n",
    "\n",
    "# Process all tweets and create a list of all words\n",
    "all_words = []\n",
    "for tweet in reduced_df['content']:\n",
    "    if isinstance(tweet, str):\n",
    "        all_words.extend(tweet_to_words(tweet))\n",
    "\n",
    "# Determine the vocabulary size\n",
    "vocabulary_size = len(set(all_words))\n",
    "print(\"Total Vocabulary Size:\", vocabulary_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing topic modeling using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections.abc import Iterable  # For checking if the value is an iterable\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Function to join tokens and handle non-iterable values\n",
    "def join_tokens(tokens):\n",
    "    if isinstance(tokens, Iterable) and not isinstance(tokens, str):  # Check if tokens is an iterable but not a string\n",
    "        return ' '.join(tokens)\n",
    "    return ''  # Replace non-iterable values with an empty string\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "reduced_df['processed_text'] = reduced_df['processed_content'].apply(join_tokens)\n",
    "\n",
    "# Example of further code\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = count_vectorizer.fit_transform(reduced_df['processed_text'])\n",
    "\n",
    "num_topics = 10\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Function to display the top words in each topic\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d:\" % (topic_idx + 1)] = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return topic_dict\n",
    "\n",
    "# Displaying the top words for each topic\n",
    "no_top_words = 10\n",
    "topic_words = display_topics(lda, count_vectorizer.get_feature_names_out(), no_top_words)\n",
    "print(topic_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `dtm` is the document-term matrix for the comments\n",
    "# and `lda` is the trained LDA model\n",
    "topic_distributions = lda.transform(dtm)\n",
    "\n",
    "# The indices of topics of interest\n",
    "topics_of_interest = [7, 4, 8, 1]  # these indices are one less than the topic numbers because of 0-indexing\n",
    "\n",
    "# Filter the topic distributions to only include the four topics of interest\n",
    "filtered_distributions = topic_distributions[:, topics_of_interest]\n",
    "\n",
    "# Get the index of the topic with the max probability for each comment\n",
    "# The index will correspond to the position in topics_of_interest\n",
    "topic_indices = filtered_distributions.argmax(axis=1)\n",
    "\n",
    "# Map the index to the class names\n",
    "class_names = {0: \"app functionality\", 1: \"customer service\", 2: \"user friendly\", 3: \"payment experience\"}\n",
    "\n",
    "# Use the mapping to translate indices to class names\n",
    "reduced_df['topic_label'] = [class_names[index] for index in topic_indices]\n",
    "\n",
    "# Now reduced_df will have a new column 'topic_label' with class names as labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reduced_df[['content', 'topic_label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating word2vec nad spling the data\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def vectorize_text(word_list, model):\n",
    "    # Remove out-of-vocabulary words\n",
    "    word_list = [word for word in word_list if word in model.wv.key_to_index]\n",
    "    \n",
    "    # If no words are left in the sentence after filtering, return a zero vector\n",
    "    if len(word_list) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Compute the average Word2Vec for the remaining words\n",
    "    word_vectors = [model.wv[word] for word in word_list]\n",
    "    vectorized_text = np.mean(word_vectors, axis=0)\n",
    "    return vectorized_text\n",
    "\n",
    "# Fill NaN values with an empty list in 'processed_content' column\n",
    "reduced_df['processed_content'] = reduced_df['processed_content'].apply(lambda x: [] if isinstance(x, float) else x)\n",
    "\n",
    "# Train the Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=reduced_df['processed_content'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Apply the vectorization function to each row in the DataFrame\n",
    "X = np.array([vectorize_text(tokens, word2vec_model) for tokens in reduced_df['processed_content']])\n",
    "\n",
    "# Label encoding for the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(reduced_df['category'])\n",
    "\n",
    "# Splitting the data into train, validation, and test sets with stratification\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Now, X_train, X_val, X_test, y_train, y_val, and y_test are ready for further use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the datasets\n",
    "print(\"Training Set Shape (X_train, y_train):\", X_train.shape, y_train.shape)\n",
    "print(\"Validation Set Shape (X_val, y_val):\", X_val.shape, y_val.shape)\n",
    "print(\"Test Set Shape (X_test, y_test):\", X_test.shape, y_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying word2vec result\n",
    "# Create a new DataFrame with the vector data\n",
    "df_vectors = pd.DataFrame(X, columns=[f'vector_{i}' for i in range(X.shape[1])])\n",
    "\n",
    "# Ensure that the length of reduced_df matches the length of X\n",
    "if len(reduced_df) == len(X):\n",
    "    for i in range(X.shape[1]):\n",
    "        reduced_df[f'vector_{i}'] = X[:, i]\n",
    "\n",
    "# Print the first few rows\n",
    "print(\"First few rows of the DataFrame with Word2Vec vectors:\")\n",
    "df_vectors.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 21688\n",
    "max_len = 100\n",
    "\n",
    "def tokenize_pad_sequences(text):\n",
    "    '''\n",
    "    This function tokenizes the input text into sequences of integers and then\n",
    "    pads each sequence to the same length.\n",
    "    '''\n",
    "    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    # Transforms text to a sequence of integers\n",
    "    X = tokenizer.texts_to_sequences(text)\n",
    "    # Pad sequences to the same length\n",
    "    X = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "    return X, tokenizer\n",
    "\n",
    "# Ensure that the DataFrame is not empty and has enough rows\n",
    "if not reduced_df.empty and len(reduced_df) > 5:\n",
    "    print('Before Tokenization & Padding \\n', reduced_df['content'].iloc[5])\n",
    "    X, tokenizer = tokenize_pad_sequences(reduced_df['content'])\n",
    "    print('After Tokenization & Padding \\n', X[5])\n",
    "else:\n",
    "    print(\"DataFrame is empty or does not have enough rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing SMOTE analysis\n",
    "# Assuming X_train, y_train are your training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Now, X_train_smote and y_train_smote are your new training sets with balanced classes\n",
    "\n",
    "# Displaying the shape of the datasets\n",
    "print(\"Original Training Set Shape:\", X_train.shape, y_train.shape)\n",
    "print(\"SMOTE Resampled Training Set Shape:\", X_train_smote.shape, y_train_smote.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking distribution of smote data\n",
    "import numpy as np\n",
    "\n",
    "unique, counts = np.unique(y_train_smote, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "print(\"Class distribution in y_train_smote:\", class_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "class F1Score(Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super(F1Score, self).__init__(name=name, **kwargs)\n",
    "        self.precision = Precision()\n",
    "        self.recall = Recall()\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "\n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.precision.reset_states()\n",
    "        self.recall.reset_states()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.optimizers import SGD as LegacySGD\n",
    "from keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "# Model parameters\n",
    "embedding_size = 100\n",
    "max_len = 100\n",
    "epochs = 10\n",
    "learning_rate = 0.1\n",
    "momentum = 0.8\n",
    "# SGD Optimizer\n",
    "sgd = LegacySGD(learning_rate=learning_rate, momentum=momentum, nesterov=False)\n",
    "# Learning Rate Scheduler\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 5 == 0 and epoch != 0:\n",
    "        return lr * 0.1\n",
    "    else:\n",
    "        return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "# Early Stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "# Building the Model with two BiLSTM layers\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu', input_shape=(max_len, 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(32)))  \n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(3, activation='softmax'))  # Adjust the number of units to match the number of classes\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_smote, y_train_smote,\n",
    "                    batch_size=32,  # Ensure this matches the actual batch size\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    callbacks=[lr_scheduler, early_stopping],\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting training and validation accuracy\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting training and validation loss\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_val is your validation data and is already preprocessed\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # Convert predictions to class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_val are the true labels\n",
    "confusion_mtx = confusion_matrix(y_val, y_pred_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap=\"Blues\")\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "f1 = f1_score(y_val, y_pred_classes, average='weighted')\n",
    "precision = precision_score(y_val, y_pred_classes, average='weighted')\n",
    "recall = recall_score(y_val, y_pred_classes, average='weighted')\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your desired path\n",
    "model_save_path = 'CNN_BiLSTM.h5'  # Replace with your desired path\n",
    "\n",
    "# Save the model in HDF5 format to the specified path\n",
    "model.save(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_test and y_test are your preprocessed test data and labels\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define the path where the model is saved\n",
    "model_save_path = 'CNN_BiLSTM.h5'\n",
    "\n",
    "# Load the model\n",
    "loaded_model = load_model(model_save_path)\n",
    "\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPOhYqBUvuCr4pz9LfJgJ9V",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
